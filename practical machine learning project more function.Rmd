---
title: "Practical Machine Learning Project"
author: "Tao Zhao"
date: "Monday, July 20, 2015"
output: html_document
---

Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

The training data for this project are from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are from https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
The data for this project come from http://groupware.les.inf.puc-rio.br/har. 

##Download and Preprocess
```{r}
library(caret)
setwd("c:/rwork/practical machine learning/project")
setwd("/home/master/rwork/practical machine learning/project")
#Download
url.train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url.test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url=url.train, destfile="pml-training.csv")
download.file(url=url.test, destfile="pml-testing.csv")
#Read data
data.train <- read.csv("pml-training.csv")
data.test <- read.csv("pml-testing.csv")
#Preprocess
#remove description columns
data.train <- data.train[, 7:ncol(data.train)]
data.test <- data.test[, 7:ncol(data.test)]
#Remove near zero variance columns
nzc <- nearZeroVar(data.train, saveMetrics = TRUE)
data.train <- data.train[,nzc$nzv==FALSE]
#data.test <- data.test[,nzc$nzv==FALSE]
#remove columns with too many NAs
nac <- apply(data.train, 2, function(x) {sum(is.na(x))})
data.train <- data.train[, which(nac < nrow(data.train)*0.7)]
#Split train data into train and testing
ti <- createDataPartition(data.train$classe, p=0.6, list=F)
model.data.train <- data.train[ti,]
model.data.test <- data.train[-ti,]
```

##Modeling

```{r}
set.seed(1)
#parallele computing for multi-core
library(doParallel)
registerDoParallel(makeCluster(detectCores()))
#Random forest with 5 fold cross validation repeated 3 times
model.rf <- train(classe ~., method="rf", data=model.data.train, trControl=trainControl(method="repeatedcv", number=5, repeats=3))
#Random forest with 10 fold cross validation
model.rf <- train(classe ~., method="rf", data=model.data.train, trControl=trainControl(method="cv", number=10))
#Random forest without cross validation
model.rf <- train(classe ~., method="rf", data=model.data.train)
#training and testing set accuracy
prediction.train <- predict(model.rf, model.data.train)
prediction.test <- predict(model.rf, model.data.test)
print(table(prediction.train, model.data.train$classe))
print(table(prediction.test, model.data.test$classe))
print(confusionMatrix(prediction.train, model.data.train$classe))
print(confusionMatrix(prediction.test, model.data.test$classe))
#Variables importance
print("variables importance in model")
vi = varImp(model.rf$finalModel)
vi$var <- rownames(vi)
vi = as.data.frame(vi[with(vi, order(vi$Overall, decreasing = TRUE, ])
rownames(vi) <- NULL
print(vi)
#Boosted trees (gbm)
model.gbm <- train(classe ~., method="gbm", data=model.data.train)
prediction.gbm.train <- predict(model.gbm, model.data.train)
prediction.gbm.test <- predict(model.gbm, model.data.test)
print(table(prediction.gbm.train, model.data.train$classe))
print(table(prediction.gbm.test, model.data.test$classe))
print(confusionMatrix(prediction.gbm.train, model.data.train$classe))
print(confusionMatrix(prediction.gbm.test, model.data.test$classe))
#Linear Discriminant Analysis (LDA) 
model.lda <- train(classe ~., method="lda", data=model.data.train)
prediction.lda.train <- predict(model.lda, model.data.train)
prediction.lda.test <- predict(model.lda, model.data.test)
print(table(prediction.lda.train, model.data.train$classe))
print(table(prediction.lda.test, model.data.test$classe))
print(confusionMatrix(prediction.lda.train, model.data.train$classe))
print(confusionMatrix(prediction.lda.test, model.data.test$classe))
#Logistic Regression (glm)--binary so doesn't fit
#Logistic Regression for multiple class prediction (mlogit)
model.logit <- mlogit(formula = classe ~.|0, data = model.data.train, method = "nr") 
#need to work on


```

##Prediction

```{r}
result.test <- predict(model.rf, data.test)
result.test
```

##Submission

```{r}
#put result to vector
answers <- as.vector(result.test)
#then load this function by copying and pasting it into R
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
#then run this line
pml_write_files(answers)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
